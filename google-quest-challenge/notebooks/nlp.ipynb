{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章のベクトル化のチュートリアル\n",
    "\n",
    "以下の5つを軽くやってみる\n",
    "\n",
    "- BoW + SVD\n",
    "- TF-IDF + word2vec\n",
    "- SWEM (fasttext)\n",
    "- Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nishikawadaiki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train = pd.read_csv(\"../dataset/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "3      3              Can an affidavit be used in Beit Din?   \n",
       "4      5       How do you make a binary image in Photoshop?   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  After playing around with macro photography on...               ysap   \n",
       "1  I am trying to understand what kinds of places...      russellpierce   \n",
       "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4  I am trying to make a binary image. I want mor...            leigero   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1024   \n",
       "1           https://rpg.stackexchange.com/users/8774   \n",
       "2  https://electronics.stackexchange.com/users/10157   \n",
       "3       https://judaism.stackexchange.com/users/5151   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  I just got extension tubes, so here's the skin...           rfusca   \n",
       "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
       "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "\n",
       "                                    answer_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1917   \n",
       "1           https://rpg.stackexchange.com/users/1871   \n",
       "2  https://electronics.stackexchange.com/users/64754   \n",
       "3       https://judaism.stackexchange.com/users/4794   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                                 url   category  ...  \\\n",
       "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS  ...   \n",
       "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE  ...   \n",
       "2  http://electronics.stackexchange.com/questions...    SCIENCE  ...   \n",
       "3  http://judaism.stackexchange.com/questions/551...    CULTURE  ...   \n",
       "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS  ...   \n",
       "\n",
       "  question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "0              1.000000        1.000000                     0.666667   \n",
       "1              0.888889        0.888889                     0.555556   \n",
       "2              0.777778        0.777778                     0.555556   \n",
       "3              0.888889        0.833333                     0.333333   \n",
       "4              1.000000        1.000000                     0.666667   \n",
       "\n",
       "   answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0          1.000000          1.000000             0.800000   \n",
       "1          0.888889          0.888889             0.666667   \n",
       "2          1.000000          1.000000             0.666667   \n",
       "3          0.833333          1.000000             0.800000   \n",
       "4          1.000000          1.000000             0.800000   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                       1.0               0.000000   \n",
       "1                       0.0               0.000000   \n",
       "2                       0.0               0.333333   \n",
       "3                       0.0               0.000000   \n",
       "4                       1.0               0.000000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.000000             1.000000  \n",
       "1                        0.666667             0.888889  \n",
       "2                        1.000000             0.888889  \n",
       "3                        1.000000             1.000000  \n",
       "4                        1.000000             1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     After playing around with macro photography on...\n",
       "1     I am trying to understand what kinds of places...\n",
       "2     I'm working on a PCB that has through-hole com...\n",
       "3     An affidavit, from what i understand, is basic...\n",
       "4     I am trying to make a binary image. I want mor...\n",
       "                            ...                        \n",
       "95    Like most people, I have a habit, and that hab...\n",
       "96    I'm working on a ERP / Accounting (lots of tab...\n",
       "97    There was the following passage in the New Yor...\n",
       "98    Ella Mental has $600$ ft of fencing to enclose...\n",
       "99    Have 12 excel Sheets in a workbook for each mo...\n",
       "Name: question_body, Length: 100, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取り扱う文章\n",
    "sentence = train['question_body']\n",
    "sentence = sentence[0:100]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW + SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoWは、sklearnのCountVectorizerを使うと良さそう\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow = count_vectorizer.fit_transform(sentence)\n",
    "\n",
    "# ngram_range: bi-gram, tri-gramができる、割と使う\n",
    "# count_vectorizer = CountVectorizer(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Bag of Words) shape :  (100, 3498)\n"
     ]
    }
   ],
   "source": [
    "print('BoW (Bag of Words) shape : ', bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>062</th>\n",
       "      <th>0x0000</th>\n",
       "      <th>0x000505b8</th>\n",
       "      <th>0x000dbfc6</th>\n",
       "      <th>0x6784fc4b</th>\n",
       "      <th>0x950e3b8d</th>\n",
       "      <th>0xb7032c3f</th>\n",
       "      <th>...</th>\n",
       "      <th>здоровье</th>\n",
       "      <th>карьера</th>\n",
       "      <th>личностный</th>\n",
       "      <th>окружение</th>\n",
       "      <th>отношения</th>\n",
       "      <th>рост</th>\n",
       "      <th>спорт</th>\n",
       "      <th>творчество</th>\n",
       "      <th>финансы</th>\n",
       "      <th>яркость</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3498 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  03  04  062  0x0000  0x000505b8  0x000dbfc6  0x6784fc4b  0x950e3b8d  \\\n",
       "0    0   0   0    0       0           0           0           0           0   \n",
       "1    0   0   0    0       0           0           0           0           0   \n",
       "2    0   0   0    1       0           0           0           0           0   \n",
       "3    0   0   0    0       0           0           0           0           0   \n",
       "4    0   0   0    0       0           0           0           0           0   \n",
       "\n",
       "   0xb7032c3f  ...  здоровье  карьера  личностный  окружение  отношения  рост  \\\n",
       "0           0  ...         0        0           0          0          0     0   \n",
       "1           0  ...         0        0           0          0          0     0   \n",
       "2           0  ...         0        0           0          0          0     0   \n",
       "3           0  ...         0        0           0          0          0     0   \n",
       "4           0  ...         0        0           0          0          0     0   \n",
       "\n",
       "   спорт  творчество  финансы  яркость  \n",
       "0      0           0        0        0  \n",
       "1      0           0        0        0  \n",
       "2      0           0        0        0  \n",
       "3      0           0        0        0  \n",
       "4      0           0        0        0  \n",
       "\n",
       "[5 rows x 3498 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=bow[0:5].toarray(),\n",
    "                  columns=count_vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TruncatedSVDを使って圧縮する\n",
    "tsvd = TruncatedSVD(n_components=50, random_state=1234)\n",
    "truncated_bow = tsvd.fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Bag of Words) shape :  (100, 50)\n"
     ]
    }
   ],
   "source": [
    "print('BoW (Bag of Words) shape : ', truncated_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まとめ\n",
    "\n",
    "class BoWVectorizerWithSVD(object):\n",
    "    def __init__(self, dims=50, random_state=1234):\n",
    "        self.count_vectorizer = None\n",
    "        self.tsvd = None\n",
    "        self.dims = dims\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # create BoW vector\n",
    "        self.count_vectorizer = CountVectorizer()\n",
    "        bow_X = self.count_vectorizer.fit_transform(X)\n",
    "        # reduce vector dimension\n",
    "        self.tsvd = TruncatedSVD(n_components=self.dims, random_state=self.random_state)\n",
    "        truncated_bow_X = self.tsvd.fit_transform(bow_X)\n",
    "\n",
    "        # return Dataframe\n",
    "        df = pd.DataFrame(data=truncated_bow_X,\n",
    "                          columns=['BoW-WithSVD-' + str(i) for i in range(self.dims)])\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        bow_X = self.count_vectorizer.transform(X)\n",
    "        truncated_bow_X = self.tsvd.transform(bow_X)\n",
    "        df = pd.DataFrame(data=truncated_bow_X,\n",
    "                          columns=['BoW-WithSVD-' + str(i) for i in range(self.dims)])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Bag of Words) shape :  (100, 50)\n"
     ]
    }
   ],
   "source": [
    "converter = BoWVectorizerWithSVD()\n",
    "truncated_bow = converter.fit_transform(sentence)\n",
    "print('BoW (Bag of Words) shape : ', truncated_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFの算出もsklearnでやる場合が多い\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vector = tfidf.fit_transform(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape :  (100, 3498)\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF shape : ', tfidf_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>062</th>\n",
       "      <th>0x0000</th>\n",
       "      <th>0x000505b8</th>\n",
       "      <th>0x000dbfc6</th>\n",
       "      <th>0x6784fc4b</th>\n",
       "      <th>0x950e3b8d</th>\n",
       "      <th>0xb7032c3f</th>\n",
       "      <th>...</th>\n",
       "      <th>здоровье</th>\n",
       "      <th>карьера</th>\n",
       "      <th>личностный</th>\n",
       "      <th>окружение</th>\n",
       "      <th>отношения</th>\n",
       "      <th>рост</th>\n",
       "      <th>спорт</th>\n",
       "      <th>творчество</th>\n",
       "      <th>финансы</th>\n",
       "      <th>яркость</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3498 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000   03   04       062  0x0000  0x000505b8  0x000dbfc6  0x6784fc4b  \\\n",
       "0  0.0  0.0  0.0  0.000000     0.0         0.0         0.0         0.0   \n",
       "1  0.0  0.0  0.0  0.000000     0.0         0.0         0.0         0.0   \n",
       "2  0.0  0.0  0.0  0.097332     0.0         0.0         0.0         0.0   \n",
       "3  0.0  0.0  0.0  0.000000     0.0         0.0         0.0         0.0   \n",
       "4  0.0  0.0  0.0  0.000000     0.0         0.0         0.0         0.0   \n",
       "\n",
       "   0x950e3b8d  0xb7032c3f  ...  здоровье  карьера  личностный  окружение  \\\n",
       "0         0.0         0.0  ...       0.0      0.0         0.0        0.0   \n",
       "1         0.0         0.0  ...       0.0      0.0         0.0        0.0   \n",
       "2         0.0         0.0  ...       0.0      0.0         0.0        0.0   \n",
       "3         0.0         0.0  ...       0.0      0.0         0.0        0.0   \n",
       "4         0.0         0.0  ...       0.0      0.0         0.0        0.0   \n",
       "\n",
       "   отношения  рост  спорт  творчество  финансы  яркость  \n",
       "0        0.0   0.0    0.0         0.0      0.0      0.0  \n",
       "1        0.0   0.0    0.0         0.0      0.0      0.0  \n",
       "2        0.0   0.0    0.0         0.0      0.0      0.0  \n",
       "3        0.0   0.0    0.0         0.0      0.0      0.0  \n",
       "4        0.0   0.0    0.0         0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 3498 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=tfidf_vector[0:5].toarray(),\n",
    "                  columns=tfidf.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecの使い方\n",
    "# size (100): 圧縮次元数\n",
    "# min_count (5): 出現頻度の低いものをカットする\n",
    "# window (5): 前後の単語を拾う際の窓の広さを決める\n",
    "# iter: 機械学習の繰り返し回数, 十分学習できていないときにこの値を調整する\n",
    "\n",
    "# 文章の分かち書きを得る\n",
    "words = [nltk.word_tokenize(val) for val in sentence] \n",
    "# 何かしらのデータで事前学習させる\n",
    "model = Word2Vec(words)\n",
    "\n",
    "# brownと言われる文章を使う場合\n",
    "# model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの辞書を作る\n",
    "word2vector = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# tf-idfの辞書を作る\n",
    "word2weight = defaultdict(lambda: max_idf, \n",
    "                          [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_embedding(sentence, word2vector, word2weight):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    vectors = np.zeros((len(words), 100))\n",
    "    \n",
    "    total_weight = 0\n",
    "    for i, val in enumerate(words):\n",
    "        try:\n",
    "            vectors[i] = word2vector[val] * word2weight[val]\n",
    "            total_weight += word2weight[val]\n",
    "        except:\n",
    "            vectors[i] = np.zeros(100)\n",
    "\n",
    "    return np.sum(vectors, axis=0) / total_weight\n",
    "\n",
    "vector = get_word_embedding(sentence[0], word2vector, word2weight)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vector=None, dims=100, use_word2vec=True):\n",
    "        # receive pre-trained model\n",
    "        self.word2vector = word2vector\n",
    "        self.word2weight = None\n",
    "        self.dims = dims\n",
    "        self.use_word2vec = use_word2vec\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # calculate tf-idf\n",
    "        tfidf = TfidfVectorizer(tokenizer=lambda x: nltk.word_tokenize(x))\n",
    "        tfidf.fit(X)\n",
    "        # create word2weight dict\n",
    "        self.word2weight = {}\n",
    "        for w, i in tfidf.vocabulary_.items():\n",
    "            self.word2weight[w] = tfidf.idf_[i]\n",
    "\n",
    "        # create word2vec dict\n",
    "        if self.word2vector is None:\n",
    "            words_list = [nltk.word_tokenize(sentence) for sentence in X]\n",
    "            model = Word2Vec(words_list, size=self.dims, seed=1234) \\\n",
    "                if self.use_word2vec else FastText(words_list, size=self.dims, seed=1234)\n",
    "            self.word2vector = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "        feature = np.array([self.get_sentence_vector(words) for words in words_list])\n",
    "        df = pd.DataFrame(data=feature,\n",
    "                          columns=['Tfidf-with-wordvec' + str(i) for i in range(feature.shape[1])])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        words_list = [nltk.word_tokenize(sentence) for sentence in X]\n",
    "        feature = np.array([self.get_sentence_vector(words) for words in words_list])\n",
    "        df = pd.DataFrame(data=feature,\n",
    "                          columns=['Tfidf-with-wordvec' + str(i) for i in range(feature.shape[1])])\n",
    "        return df\n",
    "\n",
    "    def get_sentence_vector(self, words):\n",
    "        dims = len(list(self.word2vector.values())[0])\n",
    "        vectors = np.zeros((len(words), dims))\n",
    "        total_weight = 0\n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                vectors[i] = self.word2vector[word] * self.word2weight[word]\n",
    "                total_weight += self.word2weight[word]\n",
    "            except:  # noqa\n",
    "                # doesn't find the vector\n",
    "                vectors[i] = np.zeros(dims)\n",
    "\n",
    "        return np.sum(vectors, axis=0) / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf + word2vec shape :  (100, 50)\n"
     ]
    }
   ],
   "source": [
    "converter = TfidfEmbeddingVectorizer(dims=50)\n",
    "tfidf_word2vec_vector = converter.fit_transform(sentence)\n",
    "print('tfidf + word2vec shape : ', tfidf_word2vec_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWEM + FastText\n",
    "\n",
    "慣れてきたので、いきなりクラスで実装する。MAXよりMEANの方が良さそう...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEMEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vector=None, dims=100, use_word2vec=True, pooling='mean'):\n",
    "        # receive pre-trained model\n",
    "        self.word2vector = word2vector\n",
    "        self.word2weight = None\n",
    "        self.dims = dims\n",
    "        self.use_word2vec = use_word2vec\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # word2vec\n",
    "        if self.word2vector is None:\n",
    "            words_list = [nltk.word_tokenize(sentence) for sentence in X]\n",
    "            model = Word2Vec(words_list, size=self.dims, seed=1234) \\\n",
    "                if self.use_word2vec else FastText(words_list, size=self.dims, seed=1234)\n",
    "            self.word2vector = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "        # create features\n",
    "        feature = np.array([self.get_sentence_vector(words) for words in words_list])\n",
    "        df = pd.DataFrame(data=feature,\n",
    "                          columns=['SWEM-' + str(i) for i in range(feature.shape[1])])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        words_list = [nltk.word_tokenize(sentence) for sentence in X]\n",
    "        feature = np.array([self.get_sentence_vector(words) for words in words_list])\n",
    "        df = pd.DataFrame(data=feature,\n",
    "                          columns=['SWEM-' + str(i) for i in range(feature.shape[1])])\n",
    "        return df\n",
    "\n",
    "    def get_sentence_vector(self, words):\n",
    "        dims = len(list(self.word2vector.values())[0])\n",
    "        vectors = np.zeros((len(words), dims))\n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                vectors[i] = self.word2vector[word]\n",
    "            except:  # noqa\n",
    "                # doesn't find the vector\n",
    "                vectors[i] = np.zeros(dims)\n",
    "\n",
    "        # only max, min, mean pooling\n",
    "        if self.pooling == 'max':\n",
    "            return np.max(vectors, axis=0)\n",
    "        elif self.pooling == 'min':\n",
    "            return np.min(vectors, axis=0)\n",
    "        elif self.pooling == 'mean':\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWEM shape :  (100, 50)\n"
     ]
    }
   ],
   "source": [
    "converter = SWEMEmbeddingVectorizer(dims=50, use_word2vec=False)\n",
    "swem_vector = converter.fit_transform(sentence)\n",
    "print('SWEM shape : ', swem_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecVectorizer(object):\n",
    "    def __init__(self, dims=100):\n",
    "        self.model = None\n",
    "        self.dims = dims\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        tagged_data = [TaggedDocument(words=nltk.word_tokenize(_d.lower()), tags=[str(i)])\n",
    "                       for i, _d in enumerate(X)]\n",
    "        self.model = Doc2Vec(tagged_data, vector_size=self.dims)\n",
    "        feature = np.array([self.model.docvecs[i] for i in range(len(X))])\n",
    "        df = pd.DataFrame(data=feature,\n",
    "                          columns=['doc2vec-' + str(i) for i in range(self.dims)])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        words_list = [nltk.word_tokenize(sentence.lower()) for sentence in X]\n",
    "        feature = np.array([self.model.infer_vector(words) for words in words_list])\n",
    "        df = pd.DataFrame(data=feature,\n",
    "                          columns=['doc2vec-' + str(i) for i in range(self.dims)])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec shape :  (100, 100)\n"
     ]
    }
   ],
   "source": [
    "converter = Doc2VecVectorizer()\n",
    "doc2vec_vector = converter.fit_transform(sentence)\n",
    "print('doc2vec shape : ', doc2vec_vector.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}